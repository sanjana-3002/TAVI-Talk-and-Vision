# TAVI â€” Talkative Vision Assistant (for the Visually Impaired)

**TAVI** is an AI-powered, multimodal assistant that gives **real-time situational awareness** to visually impaired users. It combines **voice**, **video**, and an **accessible chat-style UI** to support everyday navigation, safety, and confidence.

> Built at the **Hack for Accessibility** hackathon (Google Ã— Code Your Dreams Ã— Deaf Kids Code) by a cross-functional team.

---

## ğŸš€ Overview

- **Purpose:** Turn audio + video input into clear, spoken feedback.
- **How it works:** A wake word starts listening. TAVI transcribes speech, detects intent, optionally records a short video, understands the scene (caption/OCR), and **speaks back** a concise answer.
- **Modes:**  
  - **Record** â†’ capture frames â†’ **BLIP captions** + **OCR** â†’ LLM summary â†’ **TTS**  
  - **General** â†’ direct LLM reply (no video)

---

## âœ¨ Key Features

- **ğŸ™ Always-on Voice**
  - Wake-word via **Porcupine** (â€œHello Assistantâ€, â€œI am done Assistantâ€)
  - **Whisper** STT â†’ text
  - LLM intent classification (Record / General / Fallback / TAVI-specific)

- **ğŸ“· Multimodal Perception**
  - **BLIP** for image captioning
  - **OCR** for reading text in the scene (labels, signs, screens)
  - **LLM** (ChatGroq) for short, safe summaries
  - **pyttsx3** for offline TTS

- **ğŸ“± Accessible UI**
  - **Kivy** chat layout (big type, high contrast, screen-reader-friendly)
  - Shows transcribed user query, assistant response, and media previews
  - **Continuous loop:** mic re-activates after each turn

---

## ğŸ›  Architecture

[User Voice] --(Porcupine wake)--> [Kivy Frontend]
â””â”€â”€ audio --> [Whisper STT] --> text
â”‚
â–¼
[Intent LLM]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚
[General] [Record]
â”‚ â”‚
[LLM response] [Capture frames/videos]
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ â”‚
[TTS] [BLIP captions] [OCR]
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€spoken feedback [LLM summary]
â”‚
[TTS]


- **Backend:** FastAPI (`/process_audio`, `/process_video`) orchestrates STT â†’ intent â†’ vision â†’ NLG â†’ TTS  
- **Frontend:** Kivy app (wake-word, audio capture, UI)  
- **Config:** `.env` for keys and endpoints

---

## ğŸ“¦ Tech Stack

- **Programming:** Python  
- **Frontend:** Kivy  
- **Backend:** FastAPI  
- **AI Models:** Whisper (STT), BLIP (captioning), OCR, ChatGroq LLM (reasoning/summarization), pyttsx3 (TTS)  
- **Wake Word:** Porcupine  
- **Design:** Figma / Framer prototypes for accessible flows

---

## âš™ï¸ Setup

### 1) Requirements
- Python **3.10+**
- Platform audio permissions (mic + speakers)
- (Optional) GPU for faster vision/LLM

### 2) Clone & install
```bash
git clone <YOUR_REPO_URL>
cd tavi

# (recommended) virtual env
python -m venv .venv
# Windows: .venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

pip install -r requirements.txt
```

### 3) Configure environment

Create a .env at repo root:

# Backends
BACKEND_URL=http://127.0.0.1:8000

# APIs / Keys (use a secrets manager in production)
OPENAI_API_KEY=<optional_if_used>
HUGGINGFACE_API_KEY=<key_for_BLIP_if_required>
GROQ_API_KEY=<key_for_chatgroq>
PORCUPINE_ACCESS_KEY=<porcupine_key>

# Model/runtime options
WHISPER_MODEL=base
BLIP_MODEL=Salesforce/blip-image-captioning-base


Keep keys out of source control.

### â–¶ï¸ Run

**Backend** (FastAPI)

uvicorn backend.main:app --host 0.0.0.0 --port 8000 --reload


**Frontend** (Kivy)

python frontend/app.py


Say â€œHello Assistantâ€ to start.

Speak your request (â€œWhatâ€™s in front of me?â€ / â€œRead this label.â€).

Say â€œI am done Assistantâ€ to end a turn.

## ğŸ”Œ API (Backend)

POST /process_audio
Body: raw audio or base64; returns { text, intent, response_tts }

POST /process_video
Body: frames or a short clip; returns { captions, ocr_text, summary_tts }

GET /health â†’ { status: "ok" }

Exact payloads may vary; see backend/routers/*.py for schemas.

## ğŸ§ª Tips & Testing

Test STT in quiet environments first; adjust mic gain.

Use small models (Whisper base, BLIP base) for laptops.

If TTS stutters, try pyttsx3 voice rate and volume settings.

For OCR, ensure good lighting and steady frames.

## ğŸ—ºï¸ Roadmap

Navigation tie-ins (GPS, beacons)

Wearable support (smart glasses, phone camera relay)

Better streaming (frame batching, VAD for voice)

Multilingual STT/TTS

## ğŸ† Acknowledgments

Built at the Hack for Accessibility with support from Google, Code Your Dreams, and Deaf Kids Code.

Thanks to contributors across AI/ML, frontend/backend, UX research, and accessibility testing.
